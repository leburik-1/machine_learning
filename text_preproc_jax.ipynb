{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdCBatRO3ggKeKnz5Y/WRu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leburik-1/machine_learning/blob/main/text_preproc_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LSHx-_sMERd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "e7fcbae3-c43c-4e28-8d0a-9ac96249507d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded File: ../data/timemachine.txt\n",
            "number of lines: 3221\n",
            "0 the time machine by h g wells\n",
            "1 \n",
            "2 \n",
            "3 \n",
            "4 \n",
            "5 i\n",
            "6 \n",
            "7 \n",
            "8 the time traveller for so it will be convenient to speak of him\n",
            "9 was expounding a recondite matter to us his grey eyes shone and\n",
            "10 twinkled and his usually pale face was flushed and animated the\n",
            "total num charactes  170580\n",
            "total num words  32775\n",
            "['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "['i']\n",
            "[]\n",
            "[]\n",
            "['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
            "['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
            "['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n",
            "[('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n",
            "words:  ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
            "indices: [1, 19, 50, 40, 2183, 2184, 400]\n",
            "words:  ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
            "indices: [1, 19, 71, 16, 37, 11, 115, 42, 680, 6, 586, 4, 108]\n",
            "words:  ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n",
            "indices: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]\n",
            "corpus : [3, 9, 2, 1, 3, 5, 13, 2, 1, 13, 4, 15, 9, 5, 6, 2, 1, 21, 19, 1]\n",
            "list_vocab_token_to_idx_items : [('<unk>', 0), (' ', 1), ('e', 2), ('t', 3), ('a', 4), ('i', 5), ('n', 6), ('o', 7), ('s', 8), ('h', 9)]\n",
            "vocab_idx_to_token : ['t', 'h', 'e', ' ', 't', 'i', 'm', 'e', ' ', 'm', 'a', 'c', 'h', 'i', 'n', 'e', ' ', 'b', 'y', ' ']\n",
            "[3 9 2]\n",
            "(3, 28)\n",
            "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]]\n",
            "batch:  0\n",
            "X:  [[16 17 18 19 20]\n",
            " [ 6  7  8  9 10]] \n",
            "Y: [[17 18 19 20 21]\n",
            " [ 7  8  9 10 11]]\n",
            "batch:  1\n",
            "X:  [[26 27 28 29 30]\n",
            " [21 22 23 24 25]] \n",
            "Y: [[27 28 29 30 31]\n",
            " [22 23 24 25 26]]\n",
            "batch:  2\n",
            "X:  [[ 1  2  3  4  5]\n",
            " [11 12 13 14 15]] \n",
            "Y: [[ 2  3  4  5  6]\n",
            " [12 13 14 15 16]]\n",
            "X:  [[ 2  3  4  5  6]\n",
            " [18 19 20 21 22]] \n",
            "Y: [[ 3  4  5  6  7]\n",
            " [19 20 21 22 23]]\n",
            "X:  [[ 7  8  9 10 11]\n",
            " [23 24 25 26 27]] \n",
            "Y: [[ 8  9 10 11 12]\n",
            " [24 25 26 27 28]]\n",
            "X:  [[12 13 14 15 16]\n",
            " [28 29 30 31 32]] \n",
            "Y: [[13 14 15 16 17]\n",
            " [29 30 31 32 33]]\n",
            "[('<unk>', 0), (' ', 1), ('e', 2), ('t', 3), ('a', 4), ('i', 5), ('n', 6), ('o', 7), ('s', 8), ('h', 9)]\n",
            "batch:  0\n",
            "X:  [[ 9  2  1  3  5]\n",
            " [12 14 11  2  1]] \n",
            "Y:  [[ 2  1  3  5 13]\n",
            " [14 11  2  1 17]]\n",
            "batch:  1\n",
            "X:  [[13  2  1 13  4]\n",
            " [17  4  8  1  4]] \n",
            "Y:  [[ 2  1 13  4 15]\n",
            " [ 4  8  1  4 12]]\n",
            "batch:  2\n",
            "X:  [[15  9  5  6  2]\n",
            " [12  7  6 18  3]] \n",
            "Y:  [[ 9  5  6  2  1]\n",
            " [ 7  6 18  3  9]]\n",
            "Go.\tVa !\n",
            "Hi.\tSalut !\n",
            "Run!\tCours !\n",
            "Run!\tCourez !\n",
            "Who?\tQui ?\n",
            "Wow!\tÇa alors !\n",
            "Fire!\tAu feu !\n",
            "Help!\tÀ l'\n",
            "Go.\tVa !\n",
            "Hi.\tSalut !\n",
            "Run!\tCours !\n",
            "Run!\tCourez !\n",
            "Who?\tQui ?\n",
            "Wow!\tÇa alors !\n",
            "Fire!\tAu feu !\n",
            "Help!\tÀ l'\n",
            "go .\tva !\n",
            "hi .\tsalut !\n",
            "run !\tcours !\n",
            "run !\tcourez !\n",
            "who ?\tqui ?\n",
            "wow !\tça alors !\n",
            "fire !\tau feu !\n",
            "help !\tà l'ai\n",
            " SRV-VOCAB 10012\n",
            " TARGET_VOCB 17851\n",
            "Truncate Pad - ['go', '.', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n",
            "Truncate Pad(SRC) - [47, 4, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "SRC-array shape (167130, 10)\n",
            "SRC-valid len (167130,)\n",
            "SRC ARRAY [47  4  3  1  1  1  1  1  1  1]\n",
            "SRC ARRAY 3\n",
            "X: [[14 27  4  3  1  1  1  1]\n",
            " [79  8  4  3  1  1  1  1]]\n",
            "valid lengths for X: [4 4]\n",
            "Y: [[ 20 118   5   3   1   1   1   1]\n",
            " [  0   5   3   1   1   1   1   1]]\n",
            "valid lengths for Y: [4 3]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "import collections\n",
        "import re\n",
        "import random\n",
        "import requests\n",
        "import zipfile\n",
        "import hashlib\n",
        "\n",
        "try:\n",
        "  import torch\n",
        "except ModuleNotFoundError:\n",
        "  %pip install -qq torch # Installs torch if not found\n",
        "  import torch\n",
        "from torch.utils import data # Pytorch data utilities\n",
        "\n",
        "if not os.path.exists(\"figures\"):\n",
        "  os.makedirs(\"figures\") # for saving plots\n",
        "\n",
        "\n",
        "DATA_HUB = dict()\n",
        "DATA_URL = \"http://d2l-data.s3-accelerate.amazonaws.com/\"\n",
        "DATA_HUB[\"time_machine\"] = (DATA_URL + \"timemachine.txt\",\"090b5e7e70c295757f55df93cb0a180b9691891a\")\n",
        "\n",
        "# Required functions for downloading data\n",
        "\n",
        "def download(name, cache_dir=os.path.join(\"..\", \"data\")):\n",
        "  \"\"\"Download a file inserted into DATA_HUB, return the local filename.\"\"\"\n",
        "  assert name in DATA_HUB, f\"{name} does not exist in {DATA_HUB}.\"\n",
        "  url, sha1_hash = DATA_HUB[name]\n",
        "  os.makedirs(cache_dir, exist_ok=True)\n",
        "  fname = os.path.join(cache_dir, url.split(\"/\")[-1])\n",
        "  if os.path.exists(fname):\n",
        "    sha1 = hashlib.sha1()\n",
        "    with open(fname, \"rb\") as f:\n",
        "      while True:\n",
        "        data = f.read(1048576)\n",
        "        if not data:\n",
        "          break\n",
        "        sha1.update(data)\n",
        "    if sha1.hexdigest() == sha1_hash:\n",
        "      return fname   # Hit cache\n",
        "    print(f\"Downloading {fname} from {url}...\")\n",
        "  r = requests.get(url, stream=True, verify=True)\n",
        "  with open(fname, \"wb\") as f:\n",
        "    f.write(r.content)\n",
        "  return fname\n",
        "\n",
        "fname = download(\"time_machine\")\n",
        "print(\"Downloaded File:\",fname)\n",
        "\n",
        "\n",
        "def download_extract(name, folder=None):\n",
        "  \"\"\"Download and extract a zip/tar file.\"\"\"\n",
        "  fname = download(name)\n",
        "  base_dir = os.path.dirname(fname)\n",
        "  data_dir, ext = os.path.splitext(fname)\n",
        "\n",
        "  if ext == \".zip\":\n",
        "    fp = zipfile.ZipFile(fname, \"r\")\n",
        "  elif ext in (\".tar\", \".gz\"):\n",
        "    fp = tarfile.open(fname, \"r\")\n",
        "  else:\n",
        "    assert False, \"Only zip/tar files can be extracted.\"\n",
        "  fp.extractall(base_dir)\n",
        "  return os.path.join(base_dir, folder) if folder else data_dir\n",
        "\n",
        "def read_time_machine():\n",
        "  \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n",
        "  with open(download(\"time_machine\"), \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "  return [re.sub(\"[^A-Za-z]+\", \" \", line).strip().lower() for line in lines]\n",
        "\n",
        "lines = read_time_machine()\n",
        "print(f\"number of lines: {len(lines)}\")\n",
        "\n",
        "for i in range(11):\n",
        "  print(i, lines[i])\n",
        "\n",
        "nchars = 0\n",
        "nwords = 0\n",
        "\n",
        "for i in range(len(lines)):\n",
        "  nchars += len(lines[i])\n",
        "  words = lines[i].split()\n",
        "  nwords += len(words)\n",
        "\n",
        "print(\"total num charactes \", nchars)\n",
        "print(\"total num words \", nwords)\n",
        "\n",
        "def tokenize(lins, token=\"word\"):\n",
        "  \"\"\"Split text lines into word or character tokens.\"\"\"\n",
        "  if token == \"word\":\n",
        "    return [line.split() for line in lines]\n",
        "  elif token == \"char\":\n",
        "    return [list(line) for line in lines]\n",
        "  else:\n",
        "    print(\"Error: unknown token type: \" + token)\n",
        "\n",
        "tokens = tokenize(lines)\n",
        "for i in range(11):\n",
        "  print(tokens[i])\n",
        "\n",
        "def count_corpus(tokens):\n",
        "  \"\"\"Count token frequencies.\"\"\"\n",
        "  # Here `Tokens` is a 1D list or 2D list\n",
        "  if len(tokens) == 0 or isinstance(tokens[0], list):\n",
        "    # Flatten a list of token lists into a list of tokens\n",
        "    tokens = [token for line in tokens for token in line]\n",
        "  return collections.Counter(tokens)\n",
        "\n",
        "\n",
        "class Vocab:\n",
        "  \"\"\"Vocabulary for text.\"\"\"\n",
        "\n",
        "  def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
        "    if tokens is None:\n",
        "      tokens = []\n",
        "    if reserved_tokens is None:\n",
        "      reserved_tokens = []\n",
        "\n",
        "    # Sort according to frequencies\n",
        "    counter = count_corpus(tokens)\n",
        "    self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "    # The index for the unknown token is 0\n",
        "    self.unk, uniq_tokens = 0, [\"<unk>\"] + reserved_tokens\n",
        "\n",
        "    uniq_tokens += [ token for token, freq in self.token_freqs if freq >= min_freq and token not in uniq_tokens ]\n",
        "    self.idx_to_token, self.token_to_idx = [], dict()\n",
        "    for token in uniq_tokens:\n",
        "      self.idx_to_token.append(token)\n",
        "      self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.idx_to_token)\n",
        "\n",
        "  def __getitem__(self, tokens):\n",
        "    if not isinstance(tokens, (list,tuple)):\n",
        "      return self.token_to_idx.get(tokens, self.unk)\n",
        "    return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "  def to_tokens(self, indices):\n",
        "    if not isinstance(indices, (list, tuple)):\n",
        "      return self.idx_to_token[indices]\n",
        "    return [ self.idx_to_token for index in indices ]\n",
        "\n",
        "vocab = Vocab(tokens)\n",
        "print(list(vocab.token_to_idx.items())[:10])\n",
        "\n",
        "for i in [0,8,10]:\n",
        "  print(\"words: \", tokens[i])\n",
        "  print(\"indices:\", vocab[tokens[i]])\n",
        "\n",
        "def load_corpus_time_machine(max_tokens=-1):\n",
        "  \"\"\"Return token indices and the vocabulary of the time machine dataset.\"\"\"\n",
        "  lines = read_time_machine()\n",
        "  tokens = tokenize(lines, \"char\")\n",
        "  vocab = Vocab(tokens)\n",
        "  # Since each text line in the time machine dataset is not necessarily a\n",
        "  # sentence or a paragraph, flatten all the text lines into a single list\n",
        "  corpus = [ vocab[token] for line in tokens for token in line ]\n",
        "  if max_tokens > 0:\n",
        "    corpus = corpus[:max_tokens]\n",
        "  return corpus, vocab\n",
        "\n",
        "corpus, vocab = load_corpus_time_machine()\n",
        "len(corpus), len(vocab)\n",
        "\n",
        "print(f'corpus : {corpus[:20]}')\n",
        "print(f'list_vocab_token_to_idx_items : {list(vocab.token_to_idx.items())[:10]}')\n",
        "print(f'vocab_idx_to_token : {[vocab.idx_to_token[i] for i in corpus[:20]]}')\n",
        "\n",
        "x = jnp.array(corpus[:3])\n",
        "print(x)\n",
        "X = jax.nn.one_hot(x, len(vocab))\n",
        "print(X.shape)\n",
        "print(X)\n",
        "\n",
        "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
        "  \"\"\"Generat a minibatch of subsequences using random sampling.\"\"\"\n",
        "  # Start with a random offset (inclusive of `num_steps - 1`) to partition a sequence\n",
        "  corpus = corpus[random.randint(0, num_steps - 1) :]\n",
        "  # Substract 1 since we need to account for labels\n",
        "  num_subseqs = (len(corpus) - 1) // num_steps\n",
        "  # The starting indices for subsequences of length `num_steps`\n",
        "  initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
        "\n",
        "  # In random sampling, the subsequences from two adjacent random\n",
        "  # minibatches during iteration are not necessarily adjacent on the original sequence\n",
        "  random.shuffle(initial_indices)\n",
        "\n",
        "  def data(pos):\n",
        "    # return a sequence of length `num_steps` starting from `pos`\n",
        "    return corpus[pos: pos + num_steps]\n",
        "\n",
        "  num_batches = num_subseqs // batch_size\n",
        "  for i in range(0, batch_size * num_batches, batch_size):\n",
        "    initial_indices_per_batch = initial_indices[i : i + batch_size]\n",
        "    X = [data(j) for j in initial_indices_per_batch]\n",
        "    Y = [data(j + 1) for j in initial_indices_per_batch]\n",
        "\n",
        "    yield jnp.array(X), jnp.array(Y)\n",
        "\n",
        "my_seq = list(range(35))\n",
        "b = 0\n",
        "\n",
        "for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):\n",
        "  print(\"batch: \", b)\n",
        "  print(\"X: \",X, \"\\nY:\", Y)\n",
        "  b += 1\n",
        "\n",
        "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
        "    \"\"\"Generate a minibatch of subsequences using sequential partitioning.\"\"\"\n",
        "    # Start with a random offset to partition a sequence\n",
        "    offset = random.randint(0, num_steps)\n",
        "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
        "    Xs = jnp.array(corpus[offset : offset + num_tokens])\n",
        "    Ys = jnp.array(corpus[offset + 1 : offset + 1 + num_tokens])\n",
        "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
        "    num_batches = Xs.shape[1] // num_steps\n",
        "    for i in range(0, num_steps * num_batches, num_steps):\n",
        "        X = Xs[:, i : i + num_steps]\n",
        "        Y = Ys[:, i : i + num_steps]\n",
        "        yield X, Y\n",
        "\n",
        "for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):\n",
        "  print(\"X: \", X, \"\\nY:\", Y)\n",
        "\n",
        "class SeqDataLoader:\n",
        "  \"\"\"An iterator to load sequence data.\"\"\"\n",
        "\n",
        "  def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
        "    if use_random_iter:\n",
        "      self.data_iter_fn = seq_data_iter_random\n",
        "    else:\n",
        "      self.data_iter_fn = seq_data_iter_sequential\n",
        "    self.corpus, self.vocab = load_corpus_time_machine(max_tokens)\n",
        "    self.batch_size, self.num_steps = batch_size, num_steps\n",
        "\n",
        "  def __iter__(self):\n",
        "    return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)\n",
        "\n",
        "def load_data_time_machine(batch_size, num_steps, use_random_iter=False, max_tokens=10000):\n",
        "  \"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\n",
        "  data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens)\n",
        "  return data_iter, data_iter.vocab\n",
        "\n",
        "data_iter, vocab = load_data_time_machine(2,5)\n",
        "print(list(vocab.token_to_idx.items())[:10])\n",
        "\n",
        "b = 0\n",
        "for X, Y in data_iter:\n",
        "  print(\"batch: \", b)\n",
        "  print(\"X: \", X, \"\\nY: \", Y)\n",
        "  b += 1\n",
        "  if b > 2:\n",
        "    break\n",
        "\n",
        "DATA_HUB[\"fra-eng\"] = (DATA_URL + \"fra-eng.zip\", \"94646ad1522d915e7b0f9296181140edcf86a4f5\")\n",
        "\n",
        "def read_data_nmt():\n",
        "    \"\"\"Load the English-French dataset.\"\"\"\n",
        "    data_dir = download_extract(\"fra-eng\")\n",
        "    with open(os.path.join(data_dir, \"fra.txt\"), \"r\") as f:\n",
        "        return f.read()\n",
        "\n",
        "raw_text = read_data_nmt()\n",
        "print(raw_text[:100])\n",
        "\n",
        "raw_text = read_data_nmt()\n",
        "print(raw_text[:100])\n",
        "\n",
        "def preprocess_nmt(text):\n",
        "    \"\"\"Preprocess the English-French dataset.\"\"\"\n",
        "\n",
        "    def no_space(char, prev_char):\n",
        "        return char in set(\",.!?\") and prev_char != \" \"\n",
        "\n",
        "    # Replace non-breaking space with space, and convert uppercase letters to\n",
        "    # lowercase ones\n",
        "    text = text.replace(\"\\u202f\", \" \").replace(\"\\xa0\", \" \").lower()\n",
        "    # Insert space between words and punctuation marks\n",
        "    out = [\" \" + char if i > 0 and no_space(char, text[i - 1]) else char for i, char in enumerate(text)]\n",
        "    return \"\".join(out)\n",
        "\n",
        "text = preprocess_nmt(raw_text)\n",
        "print(text[:110])\n",
        "\n",
        "def tokenize_nmt(text, num_examples=None):\n",
        "  \"\"\"Tokenize the English-French dataset.\"\"\"\n",
        "  source, target = [], []\n",
        "  for i, line in enumerate(text.split(\"\\n\")):\n",
        "    if num_examples and i > num_examples:\n",
        "      break\n",
        "    parts = line.split(\"\\t\")\n",
        "    if len(parts) == 2:\n",
        "      source.append(parts[0].split(\" \"))\n",
        "      target.append(parts[1].split(\" \"))\n",
        "\n",
        "  return source, target\n",
        "\n",
        "source, target = tokenize_nmt(text)\n",
        "source[:10], target[:10]\n",
        "\n",
        "src_vocab = Vocab(source, min_freq=2, reserved_tokens=[\"<pad>\", \"<bos>\", \"<eos>\"])\n",
        "print(f\" SRV-VOCAB {len(src_vocab)}\")\n",
        "\n",
        "# French has more high frequency words than English\n",
        "target_vocab = Vocab(target, min_freq=2, reserved_tokens=[\"<pad>\", \"<bos>\", \"<eos>\"])\n",
        "print(f\" TARGET_VOCB {len(target_vocab)}\")\n",
        "\n",
        "def truncate_pad(line, num_steps, padding_token):\n",
        "  \"\"\"Truncate or pad sequences.\"\"\"\n",
        "  if len(line) > num_steps:\n",
        "    return line[:num_steps]  # Truncate\n",
        "  return line + [padding_token] * (num_steps - len(line)) # Pad\n",
        "\n",
        "print(f'Truncate Pad - {truncate_pad(source[0], 10, \"pad\")}')\n",
        "print(f'Truncate Pad(SRC) - {truncate_pad(src_vocab[source[0]], 10, src_vocab[\"<pad>\"])}')\n",
        "\n",
        "def build_array_nmt(lines, vocab, num_steps):\n",
        "  \"\"\"Transform text sequences of machine translation into minibatches.\"\"\"\n",
        "  lines = [vocab[l] for l in lines]\n",
        "  lines = [ l + [vocab[\"<eos>\"]] for l in lines]\n",
        "  array = torch.tensor([ truncate_pad(l, num_steps, vocab[\"<pad>\"]) for l in lines ])\n",
        "  valid_len = ( array != vocab[\"<pad>\"] ).type(torch.int32).sum(1)\n",
        "  return array, valid_len\n",
        "\n",
        "num_steps = 10\n",
        "src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\n",
        "print(f'SRC-array shape {jnp.array(src_array).shape}')\n",
        "print(f'SRC-valid len {jnp.array(src_valid_len).shape}')\n",
        "\n",
        "print(f'SRC ARRAY {jnp.array(src_array[0, :])}')\n",
        "print(f'SRC ARRAY {jnp.array(src_valid_len[0])}')\n",
        "\n",
        "def load_array(data_arrays, batch_size, is_train=True):\n",
        "  \"\"\"Construct a PyTorch data iterator.\"\"\"\n",
        "  dataset = data.TensorDataset(*data_arrays)\n",
        "  return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
        "\n",
        "def load_data_nmt(batch_size, num_steps, num_examples=600):\n",
        "  \"\"\"Return the iterator and the vocabularies of the translation dataset.\"\"\"\n",
        "  text = preprocess_nmt(read_data_nmt());\n",
        "  source, target = tokenize_nmt(text,num_examples)\n",
        "  src_vocab = Vocab(source, min_freq=2, reserved_tokens=[\"<pad>\", \"<bos>\",\"<eos>\"])\n",
        "  tgt_vocab = Vocab(target, min_freq=2, reserved_tokens=[\"<pad>\", \"<bos>\", \"<eos>\"])\n",
        "  src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\n",
        "  tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)\n",
        "  data_array = (src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
        "  data_iter = load_array(data_array, batch_size)\n",
        "  return data_iter, src_vocab, tgt_vocab\n",
        "\n",
        "\n",
        "train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=2, num_steps=8)\n",
        "for X, X_valid_len, Y, Y_valid_len in train_iter:\n",
        "  print(\"X:\", jnp.array(X).astype(jnp.int32))\n",
        "  print(\"valid lengths for X:\", jnp.array(X_valid_len))\n",
        "  print(\"Y:\", jnp.array(Y).astype(jnp.int32))\n",
        "  print(\"valid lengths for Y:\", jnp.array(Y_valid_len))\n",
        "  break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DajsQa9rcjjV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}